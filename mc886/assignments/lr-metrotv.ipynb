{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "lr-metrotv.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzaGU34nVQLN",
        "colab_type": "text"
      },
      "source": [
        "# Metro Interstate Traffic Volume \n",
        "\n",
        "Giovanne Lucas Dias Pereira Mariano, 173317, g173317@dac.unicamp.br\n",
        "\n",
        "This project is intended to find a linear regression model able to be a good predictor for the traffic volume in one interstate metro in the USA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvXY3t_8VyWb",
        "colab_type": "text"
      },
      "source": [
        "It has been developed in google colab. To access the dataset the drive with the dataset should be mounted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7MmiL5N6a8-",
        "colab_type": "code",
        "outputId": "ae093c98-a0d3-48a6-b266-ad3d7f6f07c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MamJSQgzWCbk",
        "colab_type": "text"
      },
      "source": [
        "#### 1. Imports important packages and the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "gAE1ZezW4TPh",
        "colab_type": "code",
        "outputId": "11ca9769-0e02-46c2-ae8b-19b166478a5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/linear-regression/Metro_Interstate_Traffic_Volume.csv\")\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"\",df.head())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   holiday    temp  ...            date_time  traffic_volume\n",
            "0    None  288.28  ...  2012-10-02 09:00:00            5545\n",
            "1    None  289.36  ...  2012-10-02 10:00:00            4516\n",
            "2    None  289.58  ...  2012-10-02 11:00:00            4767\n",
            "3    None  290.13  ...  2012-10-02 12:00:00            5026\n",
            "4    None  291.14  ...  2012-10-02 13:00:00            4918\n",
            "\n",
            "[5 rows x 9 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT2B9nO7WXf9",
        "colab_type": "text"
      },
      "source": [
        "#### 2. Assigning value to non-numerical features and rescaling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1olBsJY4TQL",
        "colab_type": "code",
        "outputId": "2064618c-7632-4807-f2be-ef1416910104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# function to assign a value to the non-numerical collums\n",
        "def valuation(df):\n",
        "    # the holidays are signed with 1 and the regular days with 0\n",
        "    dict_1 = {'None': 0.0, 'Columbus Day': 1.0, 'Veterans Day': 1.0, 'Thanksgiving Day': 1.0, 'Christmas Day': 1.0, \n",
        "              'New Years Day': 1.0, 'Washingtons Birthday': 1.0, 'Memorial Day': 1.0, 'Independence Day': 1.0, \n",
        "              'State Fair': 1.0, 'Labor Day': 1.0, 'Martin Luther King Jr Day': 1.0}\n",
        "    # a weight is designated to weather_main and weather description, \n",
        "    # with the most abruptal weathers receiving higher weights\n",
        "    dict_2 = {'Clear': 0.0, 'Clouds': 1.0, 'Mist': 2.0, 'Haze': 2.0, 'Fog': 2.0,  'Smoke': 2.0, \n",
        "              'Drizzle': 3.0, 'Rain': 4.0, 'Thunderstorm': 5.0, 'Snow': 5.0, 'Squall': 5.0}\n",
        "    dict_3 = {'scattered clouds': 0.0, 'broken clouds': 1.0, 'overcast clouds': 1.0, 'sky is clear': 0.0, 'few clouds': 0.0, \n",
        "              'light rain': 2.0, 'light intensity drizzle': 2.0, 'mist': 1.0, 'haze': 1.0, 'fog': 1.0, 'proximity shower rain': 3.0, \n",
        "              'drizzle': 2.0, 'moderate rain': 3.0, 'heavy intensity rain': 3.0, 'proximity thunderstorm': 4.0, \n",
        "              'thunderstorm with light rain': 4.0, 'proximity thunderstorm with rain': 4.0, 'heavy snow': 4.0, \n",
        "              'heavy intensity drizzle': 2.0, 'snow': 4.0, 'thunderstorm with heavy rain': 4.0, 'freezing rain': 3.0, \n",
        "              'shower snow': 4.0, 'light rain and snow': 4.0, 'light intensity shower rain': 3.0, 'SQUALLS': 4.0, \n",
        "              'thunderstorm with rain': 4.0, 'proximity thunderstorm with drizzle': 4.0, 'thunderstorm': 4.0, \n",
        "              'Sky is Clear': 0.0, 'very heavy rain': 3.0, 'thunderstorm with light drizzle': 4.0, 'light snow': 4.0, \n",
        "              'thunderstorm with drizzle': 4.0, 'smoke': 1.0, 'shower drizzle': 2.0, 'light shower snow': 4.0, 'sleet': 4.0}\n",
        "    # switch the values in the dictionary for the designated weights\n",
        "    for key, value in dict_1.items():\n",
        "        df.replace(key, value, inplace=True)\n",
        "    for key, value in dict_3.items():\n",
        "        df.replace(key, value, inplace=True)\n",
        "    for key, value in dict_2.items():\n",
        "        df.replace(key, value, inplace=True)\n",
        "    # extracts the schedule from the date_time, since its the important info and it allows numerical analysis\n",
        "    for i, value in enumerate(df['date_time']):\n",
        "        df.at[i,'date_time'] = float(df.at[i,'date_time'][11:13])\n",
        "    for i in range(df['traffic_volume'].size):\n",
        "        df.at[i,'traffic_volume'] = float(df.at[i,'traffic_volume'])\n",
        "    return df\n",
        "\n",
        "print(\"Printing values for festures after evaluation of the non-numerical ones:\\n\", df.head())\n",
        "df = valuation(df).values.astype(np.float32)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Printing values for festures after evaluation of the non-numerical ones:\n",
            "   holiday    temp  ...            date_time  traffic_volume\n",
            "0    None  288.28  ...  2012-10-02 09:00:00            5545\n",
            "1    None  289.36  ...  2012-10-02 10:00:00            4516\n",
            "2    None  289.58  ...  2012-10-02 11:00:00            4767\n",
            "3    None  290.13  ...  2012-10-02 12:00:00            5026\n",
            "4    None  291.14  ...  2012-10-02 13:00:00            4918\n",
            "\n",
            "[5 rows x 9 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fcc2WDkc4TQn",
        "colab_type": "code",
        "outputId": "95faab42-a87e-44a8-be55-7305c05eac02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# function to rescale all features: -0.5 < x < 0.5\n",
        "def feat_rescale(df):\n",
        "    maxx = np.max(df, axis=0)\n",
        "    half = maxx / 2\n",
        "    df = np.true_divide(df - half, maxx)*2\n",
        "    return df\n",
        "\n",
        "# randomly sort the data to split into train, validation ans test data sets\n",
        "selection = np.random.choice(range(df.shape[0]) , df.shape[0], replace=False)\n",
        "df = df[selection, :]\n",
        "X = df[:,:8]\n",
        "y = df[:,8]\n",
        "X = feat_rescale(X)\n",
        "np.set_printoptions(precision=2)\n",
        "print(\"Data after rescaling:\")\n",
        "print(\"X example:\\n\", X[10000])\n",
        "print(\"y example: \", y[10000])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data after rescaling:\n",
            "X example:\n",
            " [-1.    0.89 -1.   -1.    0.8   0.6   0.   -0.22]\n",
            "y example:  5283.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT8z4J6nW1hF",
        "colab_type": "text"
      },
      "source": [
        "#### 3. Split the data for providing results and avoid overfitting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXpnQxFY4TQ7",
        "colab_type": "code",
        "outputId": "bd931311-07fc-44f5-a99a-1ca82ced34c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "x_test = X[0:5000, :]\n",
        "x_val = X[5000:10000, :]\n",
        "x_train = X[10000:X.shape[0],:]\n",
        "\n",
        "y_test = y[0:5000]\n",
        "y_val = y[5000:10000]\n",
        "y_train = y[10000:y.shape[0]]\n",
        "print(\"Printing shape of training samples.\")\n",
        "print(\"X: (number of examples, number of features): \", x_test.shape)\n",
        "print(\"y: (number of examples,): \", y_test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Printing shape of training samples.\n",
            "X: (number of examples, number of features):  (5000, 8)\n",
            "y: (number of examples,):  (5000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNlew7GJW--n",
        "colab_type": "text"
      },
      "source": [
        "#### 4. Implementation of Mini-Batch Gradient Descent:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKjSYcZtZmm7",
        "colab_type": "text"
      },
      "source": [
        "I defined a function to compute the accuracy based on the number of predictions in a 20% acceptance range, since this would be a decent estimate for the traffic volume in a metro."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOi7Kmxm4TSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hypothesis(x, theta):\n",
        "    return np.dot(theta,x.T)\n",
        "\n",
        "# A accuracy that I found to be more meaningful than regural ones\n",
        "def accuracy(X, y, theta):\n",
        "    # the higher the difference between predicted and real values, lower the accuracy\n",
        "    summ = float(0)\n",
        "    for i in range(X.shape[0]):\n",
        "        y_pred = hypothesis(X[i], theta)\n",
        "        if np.absolute(y_pred - y[i]) < (0.2 * y[i]):\n",
        "            summ += 1\n",
        "    acc = summ/X.shape[0]\n",
        "    return acc\n",
        "\n",
        "# Gradient Descent main function\n",
        "def gradientDescent(X, y, m, theta, loss, alpha):\n",
        "    cost = 0\n",
        "    gradient = np.zeros(len(theta))\n",
        "    for i in range(m):\n",
        "        cost += np.square(hypothesis(X[i], theta) - y[i])\n",
        "        for j in range(len(theta)):\n",
        "            gradient[j] += (hypothesis(X[i], theta) - y[i]) * X[i][j]\n",
        "    cost *= 1/(2*m)\n",
        "    loss.append(cost)\n",
        "    gradient *= 1/m\n",
        "    theta -= (alpha * gradient)\n",
        "    return theta\n",
        "\n",
        "# Linear regression with Mini-Batch Gradient Descent\n",
        "def linearRegression(X, y, X_val, y_val, loss, alpha = 0.001, batch_size = 100, num_iters=10):\n",
        "    # Prepare data and parameters\n",
        "    ones = np.ones([X.shape[0],1])\n",
        "    X = np.concatenate((ones,X),axis=1)\n",
        "    ones = np.ones([X_val.shape[0],1])\n",
        "    X_val = np.concatenate((ones,X_val),axis=1)\n",
        "    theta = np.random.uniform(0, 3, size=X.shape[1])\n",
        "    # Loop through epochs and batches\n",
        "    for epoch in range(num_iters):\n",
        "        cost = []\n",
        "        for i in range(0, X.shape[0], batch_size):\n",
        "            if (i + batch_size) < X.shape[0]:\n",
        "                theta = gradientDescent(X[i:i+batch_size, :], y[i:i+batch_size], batch_size, theta, cost, alpha)\n",
        "            else:\n",
        "                theta = gradientDescent(X[X.shape[0]-batch_size:X.shape[0], :], \n",
        "                                        y[X.shape[0]-batch_size:X.shape[0]], batch_size, theta, cost, alpha)\n",
        "        loss.append(sum(cost)/len(cost))\n",
        "        if (epoch == num_iters - 1) or (epoch % 5 == 0):\n",
        "            print(\"Epoch %d: Loss %.2f\" % (epoch, loss[-1]))\n",
        "            print(\"Training accuracy = %f   -   Validation accuracy = %f\" %(accuracy(X, y, theta),\n",
        "                                                                            accuracy(X_val, y_val, theta)))\n",
        "    return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8se2WlkyF4s4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_loss(cost, alpha):\n",
        "    n_iterations = [x for x in range(1,len(cost)+1)]\n",
        "    plt.plot(n_iterations, cost, label=str(alpha))\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Cost')\n",
        "    plt.legend(loc='upper right', shadow=False, fontsize='large')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLsSELv0XJ3t",
        "colab_type": "text"
      },
      "source": [
        "#### 5. Execute GD-based solution for 3 different learning rates:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "sOqw7v5Y4TSY",
        "colab_type": "code",
        "outputId": "2a902a2d-bcba-4b59-df7f-854cc15447d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# defining diferent learning rates and setting hyper parameters\n",
        "epochs = 25\n",
        "batch_size = 256\n",
        "alpha1 = 0.001\n",
        "alpha2 = 0.01\n",
        "alpha3 = 0.1\n",
        "\n",
        "# executing the linear regression for different learning rates\n",
        "print(\"*\"*75)\n",
        "cost = []\n",
        "theta = linearRegression(x_train, y_train, x_val, y_val, cost, alpha1, batch_size, epochs)\n",
        "print(\"Theta with learning rate 0.001:\\n\", theta)\n",
        "print(\"*\"*75)\n",
        "plot_loss(cost, alpha1)\n",
        "\n",
        "cost = []\n",
        "theta = linearRegression(x_train, y_train, x_val, y_val, cost, alpha2, batch_size, epochs)\n",
        "print(\"Theta with learning rate 0.01:\\n\", theta)\n",
        "print(\"*\"*75)\n",
        "plot_loss(cost, alpha2)\n",
        "\n",
        "cost = []\n",
        "theta = linearRegression(x_train, y_train, x_val, y_val, cost, alpha3, batch_size, epochs)\n",
        "print(\"Theta with learning rate 0.1:\\n\", theta)\n",
        "print(\"*\"*75)\n",
        "plot_loss(cost, alpha3)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***************************************************************************\n",
            "Epoch 0: Loss 4728034.91\n",
            "Training accuracy = 0.060465   -   Validation accuracy = 0.060000\n",
            "Epoch 5: Loss 1850197.16\n",
            "Training accuracy = 0.177076   -   Validation accuracy = 0.167600\n",
            "Epoch 10: Loss 1783872.88\n",
            "Training accuracy = 0.176945   -   Validation accuracy = 0.167800\n",
            "Epoch 15: Loss 1750968.70\n",
            "Training accuracy = 0.175924   -   Validation accuracy = 0.170200\n",
            "Epoch 20: Loss 1732859.96\n",
            "Training accuracy = 0.175584   -   Validation accuracy = 0.169000\n",
            "Epoch 24: Loss 1724156.48\n",
            "Training accuracy = 0.175400   -   Validation accuracy = 0.168800\n",
            "Theta with learning rate 0.001:\n",
            " [ 670.48 -673.17  604.7  -667.81 -663.22  240.25  -51.91 -193.01  830.41]\n",
            "***************************************************************************\n",
            "Epoch 0: Loss 2234251.05\n",
            "Training accuracy = 0.176500   -   Validation accuracy = 0.167400\n",
            "Epoch 5: Loss 1708522.32\n",
            "Training accuracy = 0.170872   -   Validation accuracy = 0.165200\n",
            "Epoch 10: Loss 1706680.66\n",
            "Training accuracy = 0.170270   -   Validation accuracy = 0.165800\n",
            "Epoch 15: Loss 1705671.53\n",
            "Training accuracy = 0.170375   -   Validation accuracy = 0.166200\n",
            "Epoch 20: Loss 1704819.84\n",
            "Training accuracy = 0.170375   -   Validation accuracy = 0.166600\n",
            "Epoch 24: Loss 1704226.64\n",
            "Training accuracy = 0.170401   -   Validation accuracy = 0.167400\n",
            "Theta with learning rate 0.01:\n",
            " [ 607.87 -608.31  919.28 -599.67 -579.81  225.52   95.33 -315.91 1111.93]\n",
            "***************************************************************************\n",
            "Epoch 0: Loss 1779257.37\n",
            "Training accuracy = 0.171841   -   Validation accuracy = 0.167400\n",
            "Epoch 5: Loss 1703658.82\n",
            "Training accuracy = 0.172129   -   Validation accuracy = 0.167600\n",
            "Epoch 10: Loss 1701548.11\n",
            "Training accuracy = 0.172861   -   Validation accuracy = 0.168800\n",
            "Epoch 15: Loss 1700696.91\n",
            "Training accuracy = 0.173385   -   Validation accuracy = 0.169200\n",
            "Epoch 20: Loss 1700334.54\n",
            "Training accuracy = 0.173542   -   Validation accuracy = 0.170000\n",
            "Epoch 24: Loss 1700197.02\n",
            "Training accuracy = 0.174066   -   Validation accuracy = 0.169400\n",
            "Theta with learning rate 0.1:\n",
            " [ 380.78 -540.03 2048.11 -333.19 -215.43  242.05  143.52 -393.14 1082.46]\n",
            "***************************************************************************\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAEGCAYAAABVSfMhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIpIoBR4REUmUAo+IiCRKgUdERBL1/wM4ESCQLXdmDgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RFA7tqdXXEX",
        "colab_type": "text"
      },
      "source": [
        "The analisys of accuracy and loss along the epochs demonstrate that there's no overfitting in this model, since the trainning, validation and test accuracies are very close to each other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e1iNbcxN4j1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalEquation(X, y):\n",
        "    x_transpose = np.transpose(X)\n",
        "    x_transpose_dot_x = x_transpose.dot(X)\n",
        "    temp_1 = np.linalg.inv(x_transpose_dot_x)\n",
        "    temp_2 = x_transpose.dot(y)\n",
        "    return temp_1.dot(temp_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG0LW8MO-4V0",
        "colab_type": "code",
        "outputId": "bb184f4a-170c-42c8-aff7-64be29115e84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# prepare data to compute accuracy and loss for all models\n",
        "ones = np.ones([x_test.shape[0],1])\n",
        "x_test = np.concatenate((ones,x_test),axis=1)\n",
        "ones = np.ones([x_train.shape[0],1])\n",
        "x_train = np.concatenate((ones,x_train),axis=1)\n",
        "ones = np.ones([x_val.shape[0],1])\n",
        "x_val = np.concatenate((ones,x_val),axis=1)\n",
        "\n",
        "# prints results for MBGD\n",
        "print(\"*\"*75)\n",
        "print(\"Linear Regression with Mini-Batch Gradient Descent (Learning Rate: 0.1)\")\n",
        "print(\"Mini-Batch GD loss: %.2f\" % cost[-1])\n",
        "print(\"Training accuracy = %.2f%%\\nValidation accuracy = %.2f%%\" %(accuracy(x_train, y_train, theta)*100,\n",
        "                                                                        accuracy(x_val, y_val, theta)*100))\n",
        "print(\"Test accuracy = %.2f%%\" % (accuracy(x_test, y_test, theta)*100))\n",
        "\n",
        "# compute normal equation solution and print results\n",
        "normal_theta = normalEquation(x_train, y_train)\n",
        "cost = 0\n",
        "for i in range(x_train.shape[0]):\n",
        "    cost += np.square(hypothesis(x_train[i], normal_theta) - y_train[i])\n",
        "cost /= (2*x_train.shape[0])\n",
        "print(\"*\"*75)\n",
        "print(\"Linear Regression with Normal Equation\")\n",
        "print(\"Normal equation loss: %.2f\" % cost)\n",
        "print(\"Training accuracy = %.2f%%\\nValidation accuracy = %.2f%%\" %(accuracy(x_train, y_train, normal_theta)*100,\n",
        "                                                                        accuracy(x_val, y_val, normal_theta)*100))\n",
        "print(\"Test accuracy = %.2f%%\" % (accuracy(x_test, y_test, normal_theta)*100))\n",
        "\n",
        "# compute sgdregressor solution and print results\n",
        "sgd_model = SGDRegressor(max_iter=6000, penalty=None, eta0=0.01)\n",
        "sgd_model.fit(x_train, y_train)\n",
        "sgd_theta = sgd_model.coef_\n",
        "print(\"*\"*75)\n",
        "print(\"Linear Regression with SGDRegressor\")\n",
        "print(\"Training accuracy = %.2f%%\\nValidation accuracy = %.2f%%\" %(accuracy(x_train, y_train, sgd_theta)*100,\n",
        "                                                                        accuracy(x_val, y_val, sgd_theta)*100))\n",
        "print(\"Test accuracy = %.2f%%\" % (accuracy(x_test, y_test, sgd_theta)*100))\n",
        "print(\"*\"*75)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***************************************************************************\n",
            "Linear Regression with Mini-Batch Gradient Descent (Learning Rate: 0.1)\n",
            "Mini-Batch GD loss: 1700197.02\n",
            "Training accuracy = 17.41%\n",
            "Validation accuracy = 16.94%\n",
            "Test accuracy = 17.48%\n",
            "***************************************************************************\n",
            "Linear Regression with Normal Equation\n",
            "Normal equation loss: 1697758.63\n",
            "Training accuracy = 17.40%\n",
            "Validation accuracy = 16.94%\n",
            "Test accuracy = 17.46%\n",
            "***************************************************************************\n",
            "Linear Regression with SGDRegressor\n",
            "Training accuracy = 14.82%\n",
            "Validation accuracy = 14.18%\n",
            "Test accuracy = 14.86%\n",
            "***************************************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdZ9A-u2YZbn",
        "colab_type": "text"
      },
      "source": [
        "This results shows us that normal equation and GD-based solutions are almost equivalent, as expected, while the SGDRegressor didn't reached the same resuts, possibly because the parameters hasn't been studied enough to maximize the solution, but that doesn't represent a big interfence in the conclusions. However, the linear model didn't performed as good as expectations could be. Therefore, we may conclude that the dataset doesn't present a highly linearly correlated set of features or that the valuation of non-numerical features was poorly linked with domain knowledge."
      ]
    }
  ]
}